import requests
import logging
import re
import json
from typing import Optional, List, Dict
from bs4 import BeautifulSoup, Comment

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –±–∞–∑–æ–≤–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# =========================================================
# 1. –ó–ê–ì–†–£–ó–ö–ê HTML-–ö–û–î–ê
# =========================================================

def fetch_html_content(url: str) -> Optional[str]:
    """–°–∫–∞—á–∏–≤–∞–µ—Ç HTML-–∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã."""
    try:
        logging.info(f"–ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ URL: {url}")
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, timeout=15, headers=headers)
        response.raise_for_status()
        response.encoding = response.apparent_encoding
        logging.info("SUCCESS: –°—Ç—Ä–∞–Ω–∏—Ü–∞ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")
        return response.text
    except requests.exceptions.RequestException as e:
        logging.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ {url}: {e}")
        return None

# =========================================================
# 2. –û–ß–ò–°–¢–ö–ê –¢–ï–ö–°–¢–ê
# =========================================================

def clean_html_content(html_content: str) -> str:
    """
    –û—á–∏—â–∞–µ—Ç HTML, —É–¥–∞–ª—è–µ—Ç –ª–∏—à–Ω–∏–µ —Ç–µ–≥–∏ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç.
    """
    soup = BeautifulSoup(html_content, 'lxml')
    
    # 1. –£–¥–∞–ª–µ–Ω–∏–µ —Å–ª—É–∂–µ–±–Ω—ã—Ö, –Ω–µ—Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ç–µ–≥–æ–≤ –∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤
    REMOVAL_TAGS = ['script', 'style', 'noscript', 'iframe', 'meta', 'link', 'br']
    for element in soup(REMOVAL_TAGS):
        element.decompose()
    for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):
        comment.extract()
    
    # 2. –£–¥–∞–ª–µ–Ω–∏–µ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –∏ —Ñ—É—Ç–µ—Ä–æ–≤
    elements_to_remove = [
        soup.find('div', class_='block-0-menu-16'),
        soup.find('nav', id='menu'),
        soup.find('div', id='n'),
        soup.find('header', class_='landing-header'),
        soup.find('div', class_='landing-footer'),
        soup.find('style', type='text/css'),
    ]

    for element in elements_to_remove:
        if element:
            element.decompose()
            
    # 3. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
    main_content_tag = soup.find('div', class_='landing-main')
    if not main_content_tag:
        main_content_tag = soup.body if soup.body else soup

    for tag_name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'div', 'section', 'li', 'hr', 'table']:
        for tag in main_content_tag.find_all(tag_name):
            tag.append('\n')

    pure_text = main_content_tag.get_text(separator=' ', strip=True)

    # 4. –§–∏–Ω–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–µ–ª–æ–≤
    pure_text = re.sub(r'[\s]{2,}', '\n', pure_text)
    
    return pure_text.strip()


# =========================================================
# 3. –ü–ê–†–°–ò–ù–ì –î–ê–ù–ù–´–•
# =========================================================

def get_program_level(code: str) -> str:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —É—Ä–æ–≤–µ–Ω—å –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –ø–æ –∫–æ–¥—É."""
    if re.match(r'\d{2}\.03\.\d{2}', code):
        return "–ë–∞–∫–∞–ª–∞–≤—Ä–∏–∞—Ç"
    if re.match(r'\d{2}\.05\.\d{2}', code):
        return "–°–ø–µ—Ü–∏–∞–ª–∏—Ç–µ—Ç"
    return "–î—Ä—É–≥–æ–µ"

def extract_structured_data(full_text: str) -> List[Dict]:
    """
    –ü–∞—Ä—Å–∏—Ç —Ç–µ–∫—Å—Ç –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∞–º.
    """
    # 1. –û—Ç—Å–µ–∫–∞–µ–º –Ω–∏–∂–Ω—é—é —Ç–∞–±–ª–∏—Ü—É (–∑–∞–≥–æ–ª–æ–≤–∫–∏), –∫–æ—Ç–æ—Ä–∞—è –ª–æ–º–∞–µ—Ç –ø–∞—Ä—Å–∏–Ω–≥
    pre_filter_match = re.split(r'–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏', full_text, 1, re.DOTALL)
    full_text_filtered = pre_filter_match[0].strip()
    
    # 2. –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ –±–ª–æ–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º
    program_blocks = re.findall(
        r'(\d{2}\.\d{2}\.\d{2}.*?)(?=\d{2}\.\d{2}\.\d{2}|$)', 
        full_text_filtered, re.DOTALL
    )
    
    structured_programs = []
    
    for block_content in program_blocks:
        if not block_content: 
            continue

        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –±–ª–æ–∫–∏ –±–µ–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ (–∑–∞—â–∏—Ç–∞ –æ—Ç –º—É—Å–æ—Ä–∞)
        if '–§–æ—Ä–º–∞ –æ–±—É—á–µ–Ω–∏—è' not in block_content:
            continue

        # --- –ö–æ–¥ –∏ –ù–∞–∑–≤–∞–Ω–∏–µ ---
        # –ö–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø–æ–¥-–∫–æ–¥ (XX.XX.XX.XX)
        code_match = re.search(r'^(\d{2}\.\d{2}\.\d{2}(?:\.\d{2})?)', block_content)
        program_code = code_match.group(1) if code_match else "N/A"
        
        # –ù–∞–∑–≤–∞–Ω–∏–µ: –±–µ—Ä–µ–º —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ –∫–æ–¥–∞ –¥–æ —Ñ—Ä–∞–∑—ã "–§–æ—Ä–º–∞ –æ–±—É—á–µ–Ω–∏—è"
        name_match = re.search(r'^\d{2}\.\d{2}\.\d{2}(?:\.\d{2})?\s*(.*?)(?=–§–æ—Ä–º–∞ –æ–±—É—á–µ–Ω–∏—è)', block_content, re.DOTALL)
        program_name = name_match.group(1).strip() if name_match else "N/A"
        
        # --- –§–æ—Ä–º–∞ –æ–±—É—á–µ–Ω–∏—è ---
        form_match = re.search(r'–§–æ—Ä–º–∞ –æ–±—É—á–µ–Ω–∏—è:\s*([^\s]+)', block_content)
        study_form = form_match.group(1).strip() if form_match else "N/A"

        # --- –ü—Ä–µ–¥–º–µ—Ç—ã ---
        subjects_match = re.search(
            r'–ü—Ä–µ–¥–º–µ—Ç—ã:\s*(.*?)(?=\s*–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Å—Ç:|–°—Ç–æ–∏–º–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è)', 
            block_content, re.DOTALL
        )
        subjects_str = subjects_match.group(1).strip() if subjects_match else "N/A"
        
        # –û—á–∏—Å—Ç–∫–∞ –ø—Ä–µ–¥–º–µ—Ç–æ–≤ (—É–¥–∞–ª—è–µ–º —Å–∫–æ–±–∫–∏ —Å –±–∞–ª–ª–∞–º–∏, –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏)
        subjects_clean_str = re.sub(r'\s*\([^)]*\)', '', subjects_str)
        subjects_final = re.sub(r'\s*([+/])\s*', r' \1 ', subjects_clean_str).replace('  ', ' ').strip()


        # --- –°—Ç–æ–∏–º–æ—Å—Ç—å (–†–§) ---
        # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –æ–ø–µ—á–∞—Ç–æ–∫ –≤ —Å–ª–æ–≤–µ "–¥–ª—è" (–ª–¥—è/–ø—Ä–æ–ø—É—Å–∫ —Å–ª–æ–≤–∞)
        cost_rf_match_explicit = re.search(r'–°—Ç–æ–∏–º–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è\s*(?:[^\s]+\s*)?–≥—Ä–∞–∂–¥–∞–Ω –†–§:\s*(\d+\s*\d+)', block_content)
        cost_rf = cost_rf_match_explicit.group(1).replace(' ', '') if cost_rf_match_explicit else "N/A"
        
        # –†–µ–∑–µ—Ä–≤–Ω—ã–π –ø–æ–∏—Å–∫, –µ—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —è–≤–Ω–æ–≥–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –†–§
        if cost_rf == "N/A":
            cost_rf_match_general = re.search(r'–°—Ç–æ–∏–º–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è:\s*(\d+\s*\d+)', block_content)
            if cost_rf_match_general:
                cost_rf = cost_rf_match_general.group(1).replace(' ', '')

        # --- –°—Ç–æ–∏–º–æ—Å—Ç—å (–ò–Ω–æ—Å—Ç—Ä–∞–Ω—Ü—ã) ---
        cost_foreign_match = re.search(r'–°—Ç–æ–∏–º–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∏–Ω–æ—Å—Ç—Ä–∞–Ω–Ω—ã—Ö –≥—Ä–∞–∂–¥–∞–Ω:\s*(\d+\s*\d+)', block_content)
        cost_foreign = cost_foreign_match.group(1).replace(' ', '') if cost_foreign_match else "N/A"
            
        # --- –ü—Ä–æ—Ö–æ–¥–Ω—ã–µ –±–∞–ª–ª—ã (–ò—Å—Ç–æ—Ä–∏—è) ---
        scores_match = re.search(
            r'–ü—Ä–æ—Ö–æ–¥–Ω—ã–µ –±–∞–ª–ª—ã:\s*(?:2025|‚Äî)\s*(?:2024|‚Äî)\s*(?:2023|‚Äî)\s*(?:2022|‚Äî)\s*(?:2021|‚Äî)\s*((\d+|‚Äî)\s+(\d+|‚Äî)\s+(\d+|‚Äî)\s+(\d+|‚Äî)\s+(\d+|‚Äî))',
            block_content, re.DOTALL | re.IGNORECASE
        )
        
        scores = ["N/A"] * 5
        if scores_match:
            score_values = re.findall(r'(\d+|‚Äî)', scores_match.group(1))
            if len(score_values) >= 5:
                # 2025 -> index 0, 2021 -> index 4
                scores = [s if s != '‚Äî' else "N/A" for s in score_values[:5]]


        # --- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Å—Ç ---
        quota_match = re.search(
            r'–ë—é–¥–∂–µ—Ç–Ω—ã–µ \(—Å —É—á–µ—Ç–æ–º –∫–≤–æ—Ç\)\s*–ü–ª–∞—Ç–Ω—ã–µ –¥–ª—è –≥—Ä–∞–∂–¥–∞–Ω –†–§\s*–ü–ª–∞—Ç–Ω—ã–µ –¥–ª—è –∏–Ω–æ—Å—Ç—Ä–∞–Ω–Ω—ã—Ö –≥—Ä–∞–∂–¥–∞–Ω\s*((\d+|‚Äî)\s+(\d+|‚Äî)\s+(\d+|‚Äî))', 
            block_content, re.DOTALL
        )

        places = ["0"] * 3
        if quota_match:
            place_values = re.findall(r'(\d+|‚Äî)', quota_match.group(1))
            if len(place_values) >= 3:
                places = [p if p != '‚Äî' else "0" for p in place_values[:3]]
        
        # --- –ö–≤–æ—Ç—ã ---
        separate_quota_match = re.search(r'–û—Ç–¥–µ–ª—å–Ω–∞—è –∫–≤–æ—Ç–∞:\s*(\d+|‚Äî)\s*–º–µ—Å—Ç', block_content)
        special_quota_match = re.search(r'–û—Å–æ–±–∞—è –∫–≤–æ—Ç–∞:\s*(\d+|‚Äî)\s*–º–µ—Å—Ç', block_content)
        target_quota_match = re.search(r'–¶–µ–ª–µ–≤–∞—è –∫–≤–æ—Ç–∞:\s*(\d+|‚Äî)\s*–º–µ—Å—Ç', block_content)
        
        quota_separate = separate_quota_match.group(1) if separate_quota_match and separate_quota_match.group(1) != '‚Äî' else "0"
        quota_special = special_quota_match.group(1) if special_quota_match and special_quota_match.group(1) != '‚Äî' else "0"
        quota_target = target_quota_match.group(1) if target_quota_match and target_quota_match.group(1) != '‚Äî' else "0"

        
        # –°–±–æ—Ä–∫–∞ –æ–±—ä–µ–∫—Ç–∞
        program = {
            "–ö–æ–¥": program_code,
            "–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ": program_name.strip(),
            "–§–æ—Ä–º–∞_–æ–±—É—á–µ–Ω–∏—è": study_form,
            "–ü—Ä–µ–¥–º–µ—Ç—ã": subjects_final,
            "–°—Ç–æ–∏–º–æ—Å—Ç—å_–†–§": cost_rf,
            "–°—Ç–æ–∏–º–æ—Å—Ç—å_–ò–Ω–æ—Å—Ç—Ä": cost_foreign,
            
            "–ü—Ä–æ—Ö–æ–¥–Ω–æ–π_2025": scores[0],
            "–ü—Ä–æ—Ö–æ–¥–Ω–æ–π_2024": scores[1],
            "–ü—Ä–æ—Ö–æ–¥–Ω–æ–π_2023": scores[2],
            "–ü—Ä–æ—Ö–æ–¥–Ω–æ–π_2022": scores[3],
            "–ü—Ä–æ—Ö–æ–¥–Ω–æ–π_2021": scores[4],
            
            "–ú–µ—Å—Ç–∞_–ë—é–¥–∂–µ—Ç": places[0],
            "–ú–µ—Å—Ç–∞_–ü–ª–∞—Ç–Ω—ã–µ_–†–§": places[1],
            "–ú–µ—Å—Ç–∞_–ü–ª–∞—Ç–Ω—ã–µ_–ò–Ω–æ—Å—Ç—Ä": places[2],
            
            "–ö–≤–æ—Ç–∞_–û—Ç–¥–µ–ª—å–Ω–∞—è": quota_separate,
            "–ö–≤–æ—Ç–∞_–û—Å–æ–±–∞—è": quota_special,
            "–ö–≤–æ—Ç–∞_–¶–µ–ª–µ–≤–∞—è": quota_target,
        }
        
        program["–£—Ä–æ–≤–µ–Ω—å"] = get_program_level(program_code)
        
        structured_programs.append(program)
        
    return structured_programs

# =========================================================
# 4. –ó–ê–ü–£–°–ö
# =========================================================

def parse_stankin_page(url: str):
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ç–æ—á–∫–∞ –≤—Ö–æ–¥–∞."""
    html_code = fetch_html_content(url)

    if not html_code:
        logging.error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã.")
        return

    pure_text = clean_html_content(html_code)
    structured_data = extract_structured_data(pure_text)
    
    print("\n" + "="*70)
    print(f"ü§ñ –†–ï–ó–£–õ–¨–¢–ê–¢ –ü–ê–†–°–ò–ù–ì–ê: {url}")
    print("="*70)
    
    print(json.dumps(structured_data, ensure_ascii=False, indent=4))
    print(f"\n–ò–¢–û–ì–û: –£—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω–æ {len(structured_data)} –ø—Ä–æ–≥—Ä–∞–º–º.")

if __name__ == "__main__":
    test_url = "https://priem.stankin.ru/bakalavriatispetsialitet/training_programs/"
    parse_stankin_page(test_url)